#!/bin/bash
#SBATCH --job-name=finetune_recformer_mind_optimized
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=32
#SBATCH --gpus-per-node=4
#SBATCH --mem=240G
#SBATCH --time=15:00:00
#SBATCH --partition=gpu_h100
#SBATCH --output=job_output/finetune_recformer_instruments_2_%j.out

set -e # Exit immediately if a command exits with a non-zero status.

echo "=== Job Information ==="
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Start time: $(date)"
echo "Available GPUs: $CUDA_VISIBLE_DEVICES"
echo "Available CPUs: $SLURM_CPUS_PER_TASK"
echo "Available Memory: $SLURM_MEM_PER_NODE"

module purge
module load 2024
module load Anaconda3/2024.06-1

# Activate virtual environment
source ~/recformer_env/bin/activate

# Check PyTorch and CUDA compatibility
echo "=== Environment Check ==="
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA devices: {torch.cuda.device_count()}')"

# --- Run Optimized RecFormer fine-tuning ---
echo "Starting Optimized RecFormer fine-tuning using the MIND dataset..."

# Performance optimization settings
export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export CUDA_LAUNCH_BLOCKING=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

echo "=== Training Configuration ==="
echo "Multi-GPU: 4 H100 GPUs"
echo "Batch size per GPU: 32 (Total effective: ~128 with gradient accumulation)"
echo "Max epochs: 50 (with early stopping)"
echo "Data workers: 16 per DataLoader"
echo "Preprocessing workers: 32"

python python_scripts/finetune_optimized.py \
    --pretrain_ckpt pretrained_models/recformer_seqrec_ckpt.bin \
    --data_path finetune_data/Instruments \
    --num_train_epochs 50 \
    --batch_size 32 \
    --fp16 \
    --multi_gpu \
    --gpu_ids "0,1,2,3" \
    --finetune_negative_sample_size -1 \
    --verbose 1 \
    --dataloader_num_workers 16 \
    --preprocessing_num_workers 32 \
    --pin_memory \
    --cache_item_embeddings \
    --gradient_accumulation_steps 2 \
    --learning_rate 5e-5 \
    --min_inter 0

# python python_scripts/finetune.py \
#     --pretrain_ckpt pretrained_models/recformer_seqrec_ckpt.bin \
#     --data_path finetune_data/Scientific \
#     --num_train_epochs 128 \
#     --batch_size 16 \
#     --device 3 \
#     --fp16 \
#     --finetune_negative_sample_size -1

echo "=== Job Completion ==="
echo "End time: $(date)"
echo "Job finished with exit code: $?" 