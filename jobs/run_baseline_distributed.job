#! /bin/bash
#SBATCH --job-name=run_recformer_baseline_distributed
#SBATCH --output=job_output/run_recformer_baseline_distributed_%A.out
#SBATCH --ntasks=1
#SBATCH --time=01:00:00
#SBATCH --partition=gpu_a100
#SBATCH --nodes=1
#SBATCH --cpus-per-task=32
#SBATCH --gpus=4

NPROC=4

# activate venv
mkdir -p jobs/job_output

source recformer_env/bin/activate

echo "=== Job Information ==="
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Date: $(date)"
echo "Working directory: $(pwd)"

echo "=== SLURM GPU Information ==="
echo "SLURM_GPUS: $SLURM_GPUS"
echo "SLURM_GPUS_ON_NODE: $SLURM_GPUS_ON_NODE"
echo "SLURM_GPU_BIND: $SLURM_GPU_BIND"
echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"

echo "=== System GPU Information ==="
nvidia-smi

echo "=== Starting Distributed Training ==="
echo "Using $NPROC processes for distributed training"

# Option 1: Use the new explicit distributed script
echo "Running with explicit distributed script..."
python python_scripts/run_distributed.py \
    --config_file=configs/recbole_baseline_config.yaml \
    --nproc=$NPROC

# Option 2: Use the updated original script (comment out Option 1 and uncomment this to try)
# echo "Running with updated original script..."
# python python_scripts/run.py \
#     --config_file=configs/recbole_baseline_config.yaml \
#     --nproc=$NPROC

echo "=== Training Completed ==="
echo "Final GPU status:"
nvidia-smi

echo "Job completed at: $(date)" 